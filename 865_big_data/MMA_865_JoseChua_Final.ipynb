{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMA 865, Individual Assignment 1\n",
    "\n",
    "- [Jose, Chua]\n",
    "- [20069208]\n",
    "- [January 26, 2025]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Sentiment Analysis via the ML-based approach\n",
    "\n",
    "Download the “Product Sentiment” dataset from the course portal: sentiment_train.csv and sentiment_test.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.a. Loading and Prep\n",
    "\n",
    "Load, clean, and preprocess the data as you find necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2400 entries, 0 to 2399\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Sentence  2400 non-null   object\n",
      " 1   Polarity  2400 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 37.6+ KB\n",
      "None\n",
      "                                            Sentence  Polarity\n",
      "0                           Wow... Loved this place.         1\n",
      "1                                 Crust is not good.         0\n",
      "2          Not tasty and the texture was just nasty.         0\n",
      "3  Stopped by during the late May bank holiday of...         1\n",
      "4  The selection on the menu was great and so wer...         1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2400 entries, 0 to 2399\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   Sentence  2400 non-null   object\n",
      " 1   Polarity  2400 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 37.6+ KB\n",
      "None\n",
      "                                            Sentence  Polarity\n",
      "0                           Wow... Loved this place.         1\n",
      "1                                 Crust is not good.         0\n",
      "2          Not tasty and the texture was just nasty.         0\n",
      "3  Stopped by during the late May bank holiday of...         1\n",
      "4  The selection on the menu was great and so wer...         1\n"
     ]
    }
   ],
   "source": [
    "#importing test and train data\n",
    "\n",
    "df_train = pd.read_csv(\"sentiment_train.csv\")\n",
    "\n",
    "print(df_train.info())\n",
    "print(df_train.head())\n",
    "\n",
    "df_test = pd.read_csv(\"sentiment_test.csv\")\n",
    "\n",
    "print(df_train.info())\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning**\n",
    "1. Remove Duplicates\n",
    "2. Case normalization\n",
    "3. Removing punctuation\n",
    "4. Removing special characters\n",
    "5. Removing extra whitespace\n",
    "6. removing numbers \n",
    "7. Spell Checking\n",
    "8. Tokenization\n",
    "9. Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Rows:\n",
      "                                               Sentence  Polarity\n",
      "71                                               #NAME?         1\n",
      "219                                              #NAME?         1\n",
      "814                                  I love this place.         1\n",
      "816                              The food was terrible.         0\n",
      "843                                    I won't be back.         0\n",
      "846                   I would not recommend this place.         0\n",
      "904                                              #NAME?         0\n",
      "1285                                      Great phone!.         1\n",
      "1407                                       Works great.         1\n",
      "1524                                      Works great!.         1\n",
      "1543                            Don't buy this product.         0\n",
      "1744  If you like a loud buzzing to override all you...         0\n",
      "1748                                      Does not fit.         0\n",
      "1778                              This is a great deal.         1\n",
      "1792                                       Great Phone.         1\n",
      "1892                   Excellent product for the price.         1\n",
      "1896                                       Great phone.         1\n",
      "2363                   Definitely worth checking out.           1\n"
     ]
    }
   ],
   "source": [
    "#checking for duplicates (train)\n",
    "\n",
    "duplicates = df_train[df_train['Sentence'].duplicated()]\n",
    "\n",
    "print(\"Duplicate Rows:\")\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting duplicates\n",
    "\n",
    "df_train=df_train[df_train['Sentence'] != '#NAME?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Rows:\n",
      "               Sentence  Polarity\n",
      "185  Not recommended.           0\n"
     ]
    }
   ],
   "source": [
    "#checking for duplicates (test)\n",
    "\n",
    "duplicates = df_test[df_test['Sentence'].duplicated()]\n",
    "\n",
    "print(\"Duplicate Rows:\")\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my cleaning function\n",
    "\n",
    "def preprocess_text(sentence):\n",
    "  \n",
    "    # Case normalization\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    sentence = re.sub(f\"[{string.punctuation}]\", \"\", sentence)\n",
    "\n",
    "    # Replace special characters\n",
    "    sentence = unidecode(sentence)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence).strip()\n",
    "\n",
    "    # Remove numbers\n",
    "    sentence = re.sub(r\"\\d+\", \"\", sentence)\n",
    "\n",
    "    # Spell checking\n",
    "    sentence = str(TextBlob(sentence).correct())\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(sentence)\n",
    "\n",
    "    # Stopword removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    critical_stopwords = {\n",
    "        \"not\", \"no\", \"nor\", \"never\",\n",
    "        \"but\", \"however\", \"yet\", \"though\", \"although\",\n",
    "        \"very\", \"too\", \"much\", \"more\", \"most\", \"few\", \"less\", \"least\",\n",
    "        \"all\", \"some\", \"any\", \"only\", \"every\", \"each\", \"none\",\n",
    "        \"if\", \"then\", \"when\", \"while\"\n",
    "    }\n",
    "    stop_words -= critical_stopwords\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Sentence  Polarity\n",
      "0                                        [loved, place]         1\n",
      "1                                    [crust, not, good]         0\n",
      "2                          [not, taste, texture, nasty]         0\n",
      "3     [stopped, late, may, bank, holiday, rich, stev...         1\n",
      "4                      [selection, menu, great, prices]         1\n",
      "...                                                 ...       ...\n",
      "2395  [almost, all, songs, cover, girl, oldfashioned...         0\n",
      "2396  [most, annoying, thing, cover, girl, way, rite...         0\n",
      "2397  [unfortunately, cover, girl, example, hollywoo...         0\n",
      "2398  [nonlinear, narration, thus, many, flashbacks,...         1\n",
      "2399  [good, cinematography, also, makes, monica, be...         1\n",
      "\n",
      "[2396 rows x 2 columns]\n",
      "                                              Sentence  Polarity\n",
      "0    [good, commentary, today, love, undoubtedly, f...         1\n",
      "1    [people, first, times, film, making, think, ex...         1\n",
      "2    [very, popular, when, cinema, good, house, ver...         1\n",
      "3           [feelgood, film, felt, when, came, cinema]         1\n",
      "4    [northern, humour, positive, community, repres...         1\n",
      "..                                                 ...       ...\n",
      "595  [got, bored, watching, justice, large, take, c...         0\n",
      "596  [unfortunately, any, virtue, films, production...         0\n",
      "597                               [word, embarrassing]         0\n",
      "598                               [exceptionally, bad]         0\n",
      "599  [all, all, insult, ones, intelligence, huge, w...         0\n",
      "\n",
      "[600 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Apply function to train \n",
    "df_train['Sentence'] = df_train['Sentence'].apply(preprocess_text)\n",
    "\n",
    "# Apply function to test\n",
    "df_test['Sentence'] = df_test['Sentence'].apply(preprocess_text)\n",
    "\n",
    "print(df_train)\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.b. Modeling\n",
    "\n",
    "Use your favorite ML algorithm to train a classification model.  Don’t forget everything that we’ve learned in our ML course: hyperparameter tuning, cross validation, handling imbalanced data, etc. Make reasonable decisions and try to create the best-performing classifier that you can."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***SVM***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists in the 'Sentence' column to strings\n",
    "df_train['Sentence'] = df_train['Sentence'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "df_test['Sentence'] = df_test['Sentence'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X_train = df_train['Sentence']\n",
    "y_train = df_train['Polarity']\n",
    "X_test = df_test['Sentence']\n",
    "y_test = df_test['Polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying CountVectorizer to handle stopwords\n",
    "vectorizer = CountVectorizer(stop_words=None)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear', random_state=42)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating and training the SVM model\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_model.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7533333333333333\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.84      0.76       287\n",
      "           1       0.82      0.68      0.74       313\n",
      "\n",
      "    accuracy                           0.75       600\n",
      "   macro avg       0.76      0.76      0.75       600\n",
      "weighted avg       0.76      0.75      0.75       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],  # Kernel types\n",
    "    'gamma': ['scale', 'auto'],  # Kernel coefficient\n",
    "}\n",
    "\n",
    "# Create an instance of the SVM model\n",
    "svm_model = SVC(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, \n",
    "                           cv=5, scoring='accuracy', verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(random_state=42), n_jobs=-1,\n",
       "             param_grid={'C': [0.1, 1, 10, 100], 'gamma': ['scale', 'auto'],\n",
       "                         'kernel': ['linear', 'poly', 'rbf', 'sigmoid']},\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform grid search\n",
    "grid_search.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Best Cross-Validation Accuracy: 0.815937717466945\n"
     ]
    }
   ],
   "source": [
    "# Best hyperparameters and corresponding score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.c. Assessing\n",
    "\n",
    "Use the testing data to measure the accuracy and F1-score of your model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.7566666666666667\n",
      "Test F1 Score: 0.7345454545454545\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.88      0.78       287\n",
      "           1       0.85      0.65      0.73       313\n",
      "\n",
      "    accuracy                           0.76       600\n",
      "   macro avg       0.77      0.76      0.75       600\n",
      "weighted avg       0.78      0.76      0.75       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test_vec)\n",
    "\n",
    "print(\"Test Set Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Test F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Given the accuracy and F1-score of your model, are you satisfied with the results, from a business point of view? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a business POV my SVM model's accuracy of 75.67 and F1-score of 73.45 are decent and acceptable but not entirely satisfactory for a product sentiment analysis application. In a business application I would aim for 80-90% accuracy for more reliable insights. My F1-score tells me that my model performs better at identifying negative sentiment (class 0) with higher recall (88%) however my recall for positive sentiment (class 1) is only 65% meaning my model misclassifies a significant number of positive reviews as negative. This would not be suitable from a business point of view because it makes it seem like more customers are dissatisfied than they actually are. This could lead to unnecessary product changes, misplaced marketing efforts, or incorrect conclusions about customer satisfaction. In the future I could improve my model with feature engineering such as TF-IDF vectors or n-grams, data balancing techniques or try different parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Show five example instances in which your model’s predictions were incorrect. Describe why you think the model was wrong. Don’t just guess: dig deep to figure out the root cause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Sentence  Actual  Predicted\n",
      "0  good commentary today love undoubtedly film wo...       1          1\n",
      "1  people first times film making think excellent...       1          1\n",
      "2  very popular when cinema good house very good ...       1          1\n",
      "3                feelgood film felt when came cinema       1          0\n",
      "4      northern humour positive community represents       1          1\n"
     ]
    }
   ],
   "source": [
    "#Compare predictions with actual values\n",
    "df_test_with_predictions = pd.DataFrame({\n",
    "    'Sentence': X_test,\n",
    "    'Actual': y_test,\n",
    "    'Predicted': y_pred\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "# Display the new DataFrame\n",
    "print(df_test_with_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>fat computer week unbelievable bible thumped a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>only consistent thread holding series together...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>barely boring moment film plenty humorous parts</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>riot see hugo weaving play sexobsessed gay rea...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>feelings thoughtsgabriels discomfort danceall ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence  Actual  Predicted\n",
       "307  fat computer week unbelievable bible thumped a...       0          1\n",
       "490  only consistent thread holding series together...       0          1\n",
       "242    barely boring moment film plenty humorous parts       1          0\n",
       "588  riot see hugo weaving play sexobsessed gay rea...       1          0\n",
       "433  feelings thoughtsgabriels discomfort danceall ...       1          0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_rows = df_test_with_predictions.loc[[307, 490, 242, 588, 433]]\n",
    "selected_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***1. Lack of Context Understanding***\n",
    "\n",
    "Example:\n",
    "- \"fat computer week unbelievable bible thumped adams girl actors\"\n",
    "- \"riot see hugo weaving play sexobsessed gay real estate salesman uses clients houses trusts flaming warren tom holland\"\n",
    "\n",
    "The model may be relying too heavily on individual words rather than overall sentence meaning. Words like \"unbelievable,\" \"bible thumped,\" \"fat,\" \"sexobsessed,\" and \"flaming\" might have been learned as indicators of negative sentiment, even if the sentence isn’t actually negative.\n",
    "\n",
    "\n",
    "***2. Failure to Detect Subtle Positive Sentiment***\n",
    "\n",
    "Example:\n",
    "- \"only consistent thread holding series together amazing performances lent parker anita laselva two salons quiet ideological conflict\"\n",
    "- \"barely boring moment film plenty humorous parts\"\n",
    "\n",
    "In the first sentence, the model might have focused on words like \"quiet ideological conflict\" rather than recognizing \"amazing performances\" as an indicator of positive sentiment. The second sentence contains \"barely boring\", which actually means not boring may have misclassified it as negative due to the presence of \"boring.\"\n",
    "\n",
    "***3. Ambiguous or Abstract Language***\n",
    "\n",
    "Example:\n",
    "- \"feelings thoughtsgabriels discomfort danceall intangible leap life come within viewer grasp customs portray\"\n",
    "\n",
    "This review is philosophical and vague, making it difficult for my simple SVM model to classify it correctly. This sentence contains words like discomfort which could be taken as negative sentiment. The sentence overall might be neutral or positive depending on the context (I cant even tell, thats how ambiguous this sentence is)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
